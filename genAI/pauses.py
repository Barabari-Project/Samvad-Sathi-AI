

# -------------
#  Pause analysis utilities
# -------------
#  This module analyses the pauses in the timestamp information returned by
#  Whisper / Deepgram word–level transcripts.  It now returns two additional
#  pieces of information expected by the product team:
#   • “actionable_feedback” – a concise, user-friendly coaching paragraph.
#   • “score”               – an integer in the range 1-5 indicating how well
#                             the candidate managed pauses.
#
#  The holistic score is produced by the LLM.  The exact rubric that the model
#  must follow is injected verbatim inside the prompt so that the evaluation is
#  reproducible and transparently communicated to users.
#
#    5  – Excellent pause management (<5 % long pauses, <10 % rushed, ≥15 %
#         strategic/mid-length pauses)
#    4  – Good (5-10 % long OR 10-20 % rushed, strategic ≥ 10 %)
#    3  – Fair (10-20 % long OR 20-35 % rushed, strategic < 10 %)
#    2  – Poor (>20 % long OR >35 % rushed)
#    1  – Very poor (long > 30 % or rushed > 50 %)
#
#  The rest of the dictionary structure that downstream code relies on remains
#  unchanged for backwards-compatibility (``overview``, ``details`` and
#  ``distribution``).
# ---------------------------------------------------------------------------

import json
from typing import Dict, List


# ---------------------------------------------------------------------------
# Utility helpers
# ---------------------------------------------------------------------------


def _format_ts(seconds: float) -> str:
    """Convert raw seconds to "MM:SS" string for human-friendly references."""
    mins = int(seconds // 60)
    secs = int(seconds % 60)
    return f"{mins:02d}:{secs:02d}"


def _extract_pauses(words: List[dict]) -> List[dict]:
    """Return a list of pauses with useful metadata."""
    pauses: List[Dict] = []
    for i in range(len(words) - 1):
        pause_duration = words[i + 1]["start"] - words[i]["end"]
        if pause_duration <= 0:
            # overlapping words – ignore
            continue
        pauses.append(
            {
                "index": i,
                "start": words[i]["end"],
                "end": words[i + 1]["start"],
                "duration": pause_duration,
                "before_word": words[i]["word"],
                "after_word": words[i + 1]["word"],
            }
        )
    return pauses


def analyze_pauses(asr_output: dict, call_llm):
    """Analyse pauses and generate actionable feedback.

    Parameters
    ----------
    asr_output : dict
        The *verbose_json* output of Whisper / Deepgram.  Must contain a
        top-level ``"words"`` list with ``start``, ``end`` & ``word`` keys.
    call_llm : Callable[[str], str]
        Convenience wrapper around OpenAI chat completion that the main API
        code already provides.  Must take a *prompt* and return the raw model
        response.

    Returns
    -------
    dict
        A dictionary with the following keys:

        overview        – single-sentence summary
        details         – list of granular comments with timestamps
        distribution    – % distribution of pause types
        actionable_feedback – concise coaching paragraph generated by LLM
        score           – integer (1-5) following the explicit rubric
    """

    words = asr_output.get("words", [])
    if not words:
        return {
            "overview": "No word-level timestamps provided – unable to analyse pauses.",
            "details": [],
            "distribution": {},
            "actionable_feedback": "Please re-upload the audio so that word timings are included.",
            "score": 1,
        }

    # ------------------------------------------------------------------
    # 1. Extract raw pauses ------------------------------------------------
    pauses = _extract_pauses(words)

    # ------------------------------------------------------------------
    # 2. Ask the LLM for richer linguistic context (punctuation, tags…)
    # ------------------------------------------------------------------
    # Collect a simple list of tokens to keep the prompt short.
    word_list = [w["word"] for w in words]

    llm_prompt = (
        "Analyze this interview transcript. Perform:\n"
        "1. Add punctuation at natural boundaries\n"
        "2. Identify disfluencies (fillers, repetitions)\n"
        "3. Extract important technical terms\n"
        "4. Mark sentence boundaries\n\n"
        "Return JSON with:\n"
        "- 'punctuated_text': string with punctuation\n"
        "- 'words': list of dicts with keys: 'index', 'word', 'punctuation', 'tag'\n"
        "- 'technical_terms': list of important terms\n\n"
        f"Transcript: {word_list}"
    )

    try:
        llm_output_raw = call_llm(llm_prompt)
        llm_output = json.loads(llm_output_raw)
    except Exception:
        # If the LLM call fails, continue with minimal context.
        llm_output = {}

    word_tags = {w.get("index"): w for w in llm_output.get("words", [])}

    # ------------------------------------------------------------------
    # 3. Classify pauses ---------------------------------------------------
    long_pauses: List[Dict] = []
    rushed_pauses: List[Dict] = []
    strategic_pauses: List[Dict] = []

    for pause in pauses:
        i = pause["index"]
        next_word_tag = word_tags.get(i + 1, {})

        # Long pause (> 3 s)
        if pause["duration"] > 3.0:
            long_pauses.append(pause)

        # Rushed (< 0.2 s) not at a natural boundary
        elif pause["duration"] < 0.2:
            current_tag = word_tags.get(i, {})
            if not (
                current_tag.get("punctuation") in {",", ".", "?", "!"}
                or current_tag.get("tag") == "filler"
            ):
                rushed_pauses.append(pause)

        # Strategic (0.8-1.5 s before important term/sentence start)
        elif 0.8 <= pause["duration"] <= 1.5:
            if "technical" in next_word_tag.get("tag", "") or "sentence_start" in next_word_tag.get(
                "tag", ""
            ):
                strategic_pauses.append(pause)

    # ------------------------------------------------------------------
    # 4. Build deterministic feedback (examples, distribution)  -----------
    feedback: Dict = {"overview": "", "details": [], "distribution": {}}

    templates = {
        "long": (
            long_pauses,
            "⚠️ Long pause ({duration:.1f}s) after '{before_word}' at {timestamp}: consider a short linking phrase to keep the flow.",
            f"{len(long_pauses)} overly long pauses (>3 s)",
        ),
        "rushed": (
            rushed_pauses,
            "⚠️ Rushed transition ({duration:.1f}s) between '{before_word}' → '{after_word}' at {timestamp}: add a tiny pause so listeners can follow.",
            f"{len(rushed_pauses)} rushed transitions (<0.2 s)",
        ),
        "strategic": (
            strategic_pauses,
            "✅ Good pause ({duration:.1f}s) before '{after_word}' at {timestamp}: nice emphasis.",
            f"{len(strategic_pauses)} well-placed strategic pauses",
        ),
    }

    for kind, (examples, template, summary) in templates.items():
        if not examples:
            continue
        # add up to two illustrative examples
        for ex in examples[:2]:
            # Provide human-readable timestamp at the start of the pause
            ex_with_time = {**ex, "timestamp": _format_ts(ex["start"])}
            feedback["details"].append(template.format(**ex_with_time))
        feedback["overview"] += (", " if feedback["overview"] else "") + summary

    total_pauses = len(pauses)
    if total_pauses:
        feedback["distribution"] = {
            "long": f"{len(long_pauses) / total_pauses:.1%}",
            "rushed": f"{len(rushed_pauses) / total_pauses:.1%}",
            "strategic": f"{len(strategic_pauses) / total_pauses:.1%}",
            "normal": f"{(total_pauses - len(long_pauses) - len(rushed_pauses) - len(strategic_pauses)) / total_pauses:.1%}",
        }

    if not feedback["overview"]:
        feedback["overview"] = "Good pause management overall"
        feedback["details"].append("✅ Pause patterns support clear communication")

    # ------------------------------------------------------------------
    # 5. Ask LLM for actionable feedback + score --------------------------
    # ------------------------------------------------------------------
    rubric = (
        "### Pause Management Scoring Rubric (1-5)\n"
        "5 – Excellent: <5 % long pauses AND <10 % rushed pauses AND ≥15 % strategic pauses.\n"
        "4 – Good: 5-10 % long OR 10-20 % rushed pauses; strategic ≥10 %.\n"
        "3 – Fair: 10-20 % long OR 20-35 % rushed pauses; strategic <10 %.\n"
        "2 – Poor: >20 % long OR >35 % rushed pauses.\n"
        "1 – Very poor: long pauses >30 % OR rushed pauses >50 %.\n"
    )

    stats_for_prompt = (
        f"Long pauses : {feedback['distribution'].get('long', '0%')}\n"
        f"Rushed pauses: {feedback['distribution'].get('rushed', '0%')}\n"
        f"Strategic    : {feedback['distribution'].get('strategic', '0%')}\n"
    )

    coaching_prompt = (
        "You are an interview communication coach. Use **simple, everyday language** "
        "(aim for a grade-6 reading level). Your task:\n"
        "1. Evaluate the speaker's pauses based on the stats below.\n"
        "2. Give **actionable** advice. Cite the exact word(s) and the timestamp you are referring to in parentheses so users know where to improve "
        "(e.g., after 'model' 01:22).\n"
        "3. Assign a holistic score from 1-5 following the rubric.\n\n"
        f"{rubric}\n"
        "---\n"
        "STATISTICS\n"
        f"{stats_for_prompt}\n"
        "EXAMPLE ISSUES\n"
        + "\n".join(feedback["details"]) + "\n---\n"
        "Return a JSON object with exactly these keys: 'actionable_feedback' (string) and 'score' (integer)."
    )

    actionable_feedback = "Could not generate feedback – LLM error."
    score = 3
    try:
        llm_response_raw = call_llm(coaching_prompt)
        llm_json = json.loads(llm_response_raw)
        actionable_feedback = llm_json.get("actionable_feedback", actionable_feedback)
        score = int(llm_json.get("score", score))
    except Exception:
        # If the LLM call fails or returns invalid JSON, fall back to heuristic scoring
        print("Falling to heuristic scoring")
        long_pct = float(feedback["distribution"].get("long", "0%")[:-1])
        rushed_pct = float(feedback["distribution"].get("rushed", "0%")[:-1])
        strategic_pct = float(feedback["distribution"].get("strategic", "0%")[:-1])

        # Simple heuristic consistent with rubric
        if long_pct < 5 and rushed_pct < 10 and strategic_pct >= 15:
            score = 5
        elif (5 <= long_pct < 10) or (10 <= rushed_pct < 20):
            score = 4
        elif (10 <= long_pct < 20) or (20 <= rushed_pct < 35):
            score = 3
        elif long_pct > 30 or rushed_pct > 50:
            score = 1
        else:
            score = 2

        # Provide a simple feedback sentence with reference to the first detected issue
        first_ref = feedback["details"][0] if feedback["details"] else "your answer"
        actionable_feedback = (
            f"Try to slow down or add a tiny pause {first_ref}. "
            "A quick breath before key points (about half a second) will make ideas clearer."
        )

    # Attach to feedback dict for downstream consumers
    feedback["actionable_feedback"] = actionable_feedback
    feedback["score"] = score

    return feedback

# ---------------------------------------------------------------------------
# Demonstration block (executed only when run directly) ---------------------
# ---------------------------------------------------------------------------
if __name__ == "__main__":
    import os

    sample_path = os.path.join(os.path.dirname(__file__), "../sample_jsons/transcribe_output_my_answer.json")
    if os.path.exists(sample_path):
        with open(sample_path) as _f:
            sample_json = json.load(_f)

    from openai import OpenAI
    import json
    import os

    from dotenv import load_dotenv
    load_dotenv()

    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    def call_llm(prompt: str, system:str = None,model: str = "gpt-4o-mini", temperature: float = 0.7) -> str:
        messages = []
        if system:
            messages = [{"role":"system","content":system}]
        messages.append({"role": "user", "content": prompt})
        if model == "gpt-4o-mini" or model == "gpt-4o":
            try:
                response = client.chat.completions.create(
                    model=model,
                    messages=messages
                )
                return response.choices[0].message.content.strip()
            except Exception as e:
                return f"Error in call_llm func: {e}"
        
    print(json.dumps(analyze_pauses(sample_json, call_llm), indent=2))

