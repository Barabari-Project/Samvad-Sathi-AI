# genAI Interview-Coaching Toolkit

This repository hosts a collection of **speech & text-analysis utilities** that power an
interview-coaching API.  It receives audio answers from candidates, transcribes
them, and produces actionable feedback on pacing, pauses, pronunciation, and
domain knowledge.

The project is split into small, focused modules so that individual components
can be reused or tested in isolation.  The sections below give a bird-eye view
of the directory layout and explain how the pieces interact at runtime.

---

## 1. Directory structure

```
.
â”œâ”€â”€ genAI/                         # Main Python package
â”‚   â”œâ”€â”€ server.py                  # FastAPI service â€“ single entry-point
â”‚   â”œâ”€â”€ pacing.py                  # WPM & speaking-rate analysis
â”‚   â”œâ”€â”€ pauses.py                  # Pause extraction / LLM-assisted feedback
â”‚   â”œâ”€â”€ test_pauses_samples.py     # Unit-style samples for pause logic
â”‚   â”œâ”€â”€ example.env                # Template for required env variables
â”‚   â”‚
â”‚   â”œâ”€â”€ OpenSmile_Approach/        # Paralinguistic feature extractor (openSMILE)
â”‚   â”‚   â”œâ”€â”€ app.py                 # Stand-alone script to pull eGeMAPS features
â”‚   â”‚   â””â”€â”€ *.wav / *.md           # Demo audio + output
â”‚   â”‚
â”‚   â”œâ”€â”€ misspronounciation/        # Pronunciation evaluation helpers
â”‚   â”‚   â”œâ”€â”€ main.py                # G2P + alignment proof-of-concept
â”‚   â”‚   â”œâ”€â”€ custom_lexicon.py      # Domain-specific phoneme additions
â”‚   â”‚   â””â”€â”€ ... (dataset/, audios/)# Audio assets & notebooks
â”‚   â”‚
â”‚   â”œâ”€â”€ pauses_input_samples/      # JSON snippets for manual pause testing
â”‚   â””â”€â”€ prompts/                  # LLM prompt templates & notebooks
â”‚       â”œâ”€â”€ prompts.py             # Re-usable system / user prompt strings
â”‚       â”œâ”€â”€ gen_que_prompt.py      # Question-generation helper
â”‚       â””â”€â”€ gen_que_testing/       # Sample analyses + datasets
â”‚
â”œâ”€â”€ sample_jsons/                  # End-to-end request / response examples
â”‚   â””â”€â”€ *.json
â”‚
â”œâ”€â”€ test/                          # Lightweight regression harness
â”‚   â”œâ”€â”€ testing_script.py          # Downloads audio & hits local API
â”‚   â”œâ”€â”€ Test_Cases_1.csv           # Drive links to sample answers
â”‚   â””â”€â”€ audios/                    # Small speaker excerpts for CI
â”‚
â”œâ”€â”€ requirements.txt               # Minimal runtime dependencies
â””â”€â”€ readme.md                      # ðŸ ” you are here
```

### What each key file/folder is for

| Path | Purpose |
|------|---------|
| `genAI/server.py` | Launches the FastAPI app; exposes endpoints for transcription (`/transcribe_whisper`), pacing, pause, and communication analyses.  Orchestrates calls to OpenAI, Deepgram, Sarvam.
| `genAI/pacing.py` | Sliding-window WPM computation, pace classification (too slow / ideal / too fast), and human-readable feedback generator.
| `genAI/pauses.py` | Detects silences between `words[]` timestamps, classifies them (long, rushed, strategic) and produces LLM-guided improvement tips.
| `genAI/OpenSmile_Approach/app.py` | Self-contained script that demonstrates how to extract acoustic features (eGeMAPS) via openSMILE.
| `genAI/misspronounciation/` | Experimental playground for phoneme-based pronunciation scoring using whisper + `phonemizer` alignment.
| `genAI/prompts/` | Centralised store for all system / user prompts used by the API (resume extraction, text analysis, question generation, etc.).
| `sample_jsons/` | Canonical request / response pairs that serve as documentation as well as regression fixtures.
| `test/` | Automated smoke tests â€“ download sample audio, hit local API, and collate results into a CSV report.


---

## 2. High-level code flow

1. **Incoming request**  
   A client (web or CLI) uploads an audio answer to one of the `/â€¦-analysis`
   endpoints exposed by **`server.py`**.

2. **Transcription**  
   The audio is forwarded to *Deepgram* (or Whisper) â€“ the verbose JSON
   containing word-level timestamps is cached and returned to the caller.

3. **Pace Assessment**  
   `pacing.calculate_pace_metrics()` computes WPM over 5-second hop windows,
   classifies each window, aggregates percentages, and returns granular
   segments.  `pacing.provide_pace_feedback()` converts that into a coach-style
   paragraph.

4. **Pause Analysis**  
   `pauses.analyze_pauses()` extracts silences, derives dynamic thresholds from
   the userâ€™s own speaking rate, queries an LLM for *strategic* pause
   suggestions, and outputs an overview + rubric-based score (1â€“5).

5. **Pronunciation (optional / experimental)**  
   If enabled, `misspronounciation/main.py` uses *phonemizer* + G2P alignment
   to detect word-level deviations from expected phonemes.

6. **Summary & Response**  
   `server.py` bundles the individual analyses together with auxiliary
   metadata (process time, job role prompts, etc.) and sends the JSON back to
   the frontend.


---

## 3. Running locally

```bash
# 0. Python >=3.10 recommended
$ python -m venv .venv && source .venv/bin/activate
$ pip install -r requirements.txt

# 1. Export required keys (see genAI/example.env)
$ export OPENAI_API_KEY=â€¦
$ export DEEPGRAM_API_KEY=â€¦
$ export SARVAM_API_KEY=â€¦

# 2. Fire up the API
$ uvicorn genAI.server:app --reload

# 3. Hit an endpoint
$ curl -X POST -F "file=@path/to/audio.wav" http://localhost:8000/transcribe_whisper
or
open http://localhost:8000/docs for swagger
```
